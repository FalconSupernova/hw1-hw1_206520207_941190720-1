{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM 336546 - HW1: Fetal Cardiotocograms \n",
    "# Part I: Data Exploration\n",
    "\n",
    "In this homework you will be working on predicting fetal outcomes from continuous labor monitoring using Cardiotography (CTG). In particular you will use measures of the fetal heart rate (FHR) and use these as input features to your linear classifier. Before we dive into the assignment itself, let's make a quick introduction to CTG and get familiar with this type of examination and its underlying physiological basis as a good biomedical engineer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrapartum CTG is used routinely to measure maternal uterine pressure and fetal heart rate (FHR). Antepartum CTG monitoring is used to identify fetuses at risk of intrauterine hypoxia and acidaemia. As early as 28 weeks of gestation, analysis of the FHR trace is used as a nonstress test to assess the fetal well-being. In the perinatal period, timely, appropriate intervention can avoid fetal neurological damage or death. The CTG is visually assessed by a clinician or interpreted by computer analysis. In the context of labor monitoring, the CTG is used for continuous fetal monitoring. An abnormal heart rate will lead the clinician to perform a cesarean. We will focus on CTG monitoring during labor in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CTG has two different transducers: One of them is a transducer placed on the mother’s abdomen, above the fetal heart, to monitor heart rate using Doppler probe (cardiogram). The other is located at the fundus of the uterus to measure frequency of contractions (tocogram). Tocodynamometry is a strain gauge technology provides contraction frequency and approximate duration of labor contractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a lot of features from a CTG. Here are some of them:\n",
    "* **Uterine activity**: Duration, frequency and intensity of contractions.\n",
    "* **Baseline FHR**: Mean FHR rounded to increments of 5 beats per minute (bpm) during a 10-minute window.\n",
    "* **Baseline FHR variability**: Fluctuations in the baseline FHR that are irregular in amplitude and frequency.\n",
    "* **Presence of accelerations**: A visually apparent abrupt increase in fetal heart rate.\n",
    "\n",
    "Here is an example of a typical CTG with some of its features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/6ed5ef1da100ebc2241c3a3945e1da9ce79f73ac/1-Figure1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CTG dataset is an Excel file which was sent to you. For more information, please look at the Excel sheet called Description or take a look at this [link](http://archive.ics.uci.edu/ml/datasets/Cardiotocography). Our main goal in this assignment is to train an algorithm to decide what is the fetal state according to the extracted features. Before we even start dealing with the data itself, we should apply the first and most important rule of data/signal processing: **ALWAYS LOOK AND UNDERSTAND THE DATA FIRST!**\n",
    "\n",
    "In order to do that, we will load the file into a variable called CTG_features and use descriptive statistics and visualization tools you have seen in the lectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LB - FHR baseline (beats per minute)\n",
    "AC - # of accelerations per second\n",
    "FM - # of fetal movements per second\n",
    "UC - # of uterine contractions per second\n",
    "DL - # of light decelerations per second\n",
    "DS - # of severe decelerations per second\n",
    "DP - # of prolongued decelerations per second\n",
    "ASTV - percentage of time with abnormal short term variability\n",
    "MSTV - mean value of short term variability\n",
    "ALTV - percentage of time with abnormal long term variability\n",
    "MLTV - mean value of long term variability\n",
    "Width - width of FHR histogram\n",
    "Min - minimum of FHR histogram\n",
    "Max - Maximum of FHR histogram\n",
    "Nmax - # of histogram peaks\n",
    "Nzeros - # of histogram zeros\n",
    "Mode - histogram mode\n",
    "Mean - histogram mean\n",
    "Median - histogram median\n",
    "Variance - histogram variance\n",
    "Tendency - histogram tendency\n",
    "CLASS - FHR pattern class code (1 to 10)\n",
    "NSP - fetal state class code (N=normal; S=suspect; P=pathologic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# This block makes sure the following cells are synchronized with the modules (.py files).\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Pc\\\\OneDrive - Technion\\\\Documents\\\\Machine Learning\\\\HW1\\\\hw1-hw1_206520207_941190720-1\\\\.ipynb_checkpoints\\\\messed_CTG.xls'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [5], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      6\u001B[0m file \u001B[38;5;241m=\u001B[39m Path\u001B[38;5;241m.\u001B[39mcwd()\u001B[38;5;241m.\u001B[39mjoinpath(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessed_CTG.xls\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# concatenates messed_CTG.xls to the current folder that should be the extracted zip folder \u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m CTG_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msheet_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRaw Data\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m CTG_features \u001B[38;5;241m=\u001B[39m CTG_dataset[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLB\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAC\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFM\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUC\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDL\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDS\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDR\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDP\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mASTV\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMSTV\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mALTV\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMLTV\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      9\u001B[0m  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWidth\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMin\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMax\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNmax\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNzeros\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMode\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMean\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMedian\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVariance\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTendency\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[0;32m     10\u001B[0m CTG_morph \u001B[38;5;241m=\u001B[39m CTG_dataset[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCLASS\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\bm-336546\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\bm-336546\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\bm-336546\\lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001B[0m, in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001B[0m\n\u001B[0;32m    480\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[0;32m    481\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 482\u001B[0m     io \u001B[38;5;241m=\u001B[39m \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n\u001B[0;32m    484\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    485\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    486\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    487\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\bm-336546\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[1;34m(self, path_or_buffer, engine, storage_options)\u001B[0m\n\u001B[0;32m   1650\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxls\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1651\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1652\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[43minspect_excel_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1653\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[0;32m   1654\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1656\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1657\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExcel file format cannot be determined, you must specify \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1658\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man engine manually.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1659\u001B[0m         )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\bm-336546\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001B[0m, in \u001B[0;36minspect_excel_format\u001B[1;34m(content_or_path, storage_options)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(content_or_path, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m   1523\u001B[0m     content_or_path \u001B[38;5;241m=\u001B[39m BytesIO(content_or_path)\n\u001B[1;32m-> 1525\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m   1527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[0;32m   1528\u001B[0m     stream \u001B[38;5;241m=\u001B[39m handle\u001B[38;5;241m.\u001B[39mhandle\n\u001B[0;32m   1529\u001B[0m     stream\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\bm-336546\\lib\\site-packages\\pandas\\io\\common.py:865\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    856\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    857\u001B[0m             handle,\n\u001B[0;32m    858\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    861\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    862\u001B[0m         )\n\u001B[0;32m    863\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    864\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m--> 865\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    866\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[0;32m    868\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Pc\\\\OneDrive - Technion\\\\Documents\\\\Machine Learning\\\\HW1\\\\hw1-hw1_206520207_941190720-1\\\\.ipynb_checkpoints\\\\messed_CTG.xls'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file = Path.cwd().joinpath('messed_CTG.xls') # concatenates messed_CTG.xls to the current folder that should be the extracted zip folder \n",
    "CTG_dataset = pd.read_excel(file, sheet_name='Raw Data')\n",
    "CTG_features = CTG_dataset[['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DR', 'DP', 'ASTV', 'MSTV', 'ALTV', 'MLTV',\n",
    " 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean', 'Median', 'Variance', 'Tendency']]\n",
    "CTG_morph = CTG_dataset[['CLASS']]\n",
    "fetal_state = CTG_dataset[['NSP']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First  look at the data in your Excel file. You can see that in some of the cells we have '--' or NaN etc. Furthermore, the description tells us that the feature `DR` was removed although we did load it into our dataset. Your first job is: Implement the function `rm_ext_and_nan` in the module  `clean_data` so it will remove the extra feature `DR` (ignore the feature), and **all** non-numeric values (ignore the samples). Notice that real data can have missing values in many different ways and thus your implementation has to be generic. This function should return a dictionary of features where the values of each feature are the clean excel columns without the `DR` feature. **Hint**: In order to eliminate every cell that is non-numeric, you will have to transform it first to NaN and only then eliminate them. **Note**: `CTG_dataset` is a `pandas DataFrame` and every element within it is called `pandas series`. Implement the function in a single line of code using dictionary comprehensions.\n",
    "\n",
    "A note on debugging using PyCharm (**Highly recommended**):\n",
    "* In order to fill the needed modules (i.e. the .py files functions like \"rm_ext_and_nan\" in the next block) we must open the .py files by some text editor. A simple solution would be to open it in this notebook by double-clicking on the file, but no debugging options are available. To debug the file you can open the directory of this homework as a project in PyCharm (see Moodle video), and then run the .py file in debug mode. In order to execute a function, you must prepare it's input variables beforehand. For example, in \"clean_data.py\" you will see in the \"Debugging Block!\" the same lines from the previous block (copy paste) and the next in order to run \"rm_ext_and_nan\" function successfully. To debug other functions you should do a similar procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CTG_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mclean_data\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rm_ext_and_nan \u001B[38;5;28;01mas\u001B[39;00m rm\n\u001B[0;32m      2\u001B[0m extra_feature \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDR\u001B[39m\u001B[38;5;124m'\u001B[39m \n\u001B[1;32m----> 3\u001B[0m c_ctg \u001B[38;5;241m=\u001B[39m rm(\u001B[43mCTG_features\u001B[49m, extra_feature)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'CTG_features' is not defined"
     ]
    }
   ],
   "source": [
    "from clean_data import rm_ext_and_nan as rm\n",
    "extra_feature = 'DR' \n",
    "c_ctg = rm(CTG_features, extra_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and make sure that your function works well: let's compare the histograms' width feature . First we will plot the original distribution of this feature where every NaN element was replaced by a value that is not reasonable as 1000 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "feat = 'Width'\n",
    "Q = pd.DataFrame(CTG_features[feat])\n",
    "idx_na = Q.index[Q[feat].isna() == True].tolist()\n",
    "for i in idx_na:\n",
    "    Q.loc[i] = 1000\n",
    "Q.hist(bins = 100)\n",
    "plt.xlabel('Histogram Width')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following lines of code to check how you performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'Width'\n",
    "Q_clean = pd.DataFrame(c_ctg[feat])\n",
    "Q_clean.hist(bins=100)\n",
    "plt.xlabel('Histogram Width')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are warmed up, let's do something a bit different. Instead of removing the NaN values, handle those missing values using sampling (**Tutorial C02**). Again, first convert all non-numeric values to NaN and only then apply the sampling method. Implement the function `nan2num_samp`. Don't forget to remove `DR` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import nan2num_samp\n",
    "extra_feature = 'DR' \n",
    "c_samp = nan2num_samp(CTG_features, extra_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following lines of code to check how you performed for example with the feature `MSTV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'MSTV'\n",
    "print(CTG_features[feat].iloc[0:5]) # print first 5 values\n",
    "print(c_samp[feat].iloc[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our \"clean\" data using histograms, barplots and boxplots and then refer to the following questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "c_samp.boxplot(column=['Median','Mean','Mode'])\n",
    "plt.ylabel('Fetal Heart Rate [bpm]')\n",
    "# Histograms\n",
    "xlbl = ['beats/min','1/sec','1/sec','%']\n",
    "axarr = c_samp.hist(column=['LB', 'AC', 'UC','ASTV'], bins=100,layout = (2, 2),figsize=(20, 10))\n",
    "for i,ax in enumerate(axarr.flatten()):\n",
    "    ax.set_xlabel(xlbl[i])\n",
    "    ax.set_ylabel(\"Count\")\n",
    "# Barplots (error bars)\n",
    "df = pd.DataFrame.from_dict({'lab':['Min','Max'], 'val':[np.mean(c_samp['Min']), np.mean(c_samp['Max'])]})\n",
    "errors = [np.std(c_samp['Min']), np.std(c_samp['Max'])] \n",
    "ax = df.plot.bar(x='lab', y='val', yerr=errors, rot=0)\n",
    "ax.set_ylabel('Average value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**: \n",
    "* Please answer all of the following questions within the notebook itself. Remember that the only files you will submit are the notebook and the fully-implemented `.py` files.\n",
    "* Do not change the notebook's cells unless you were specificly told to (such as the \"Answers\" cells etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q1:** What information can you get from histograms and what information can you get from boxplots?\n",
    "\n",
    "**Q2:** Error bars can be misleading. In what sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q1:** A histogram is used to summarize discrete or continuous data. In other words, it provides a visual interpretation of numerical data by showing the number of data points that fall within a specified range of values (called “bins”). A box and whisker plot—also called a box plot—displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. In a box plot, we draw a box from the first quartile to the third quartile. A vertical line goes through the box at the median. In the current problem we are finding the median of \"Median\" coulumn of dataframe.\n",
    "\n",
    "**Q2:** Error bars give a general idea of how precise a measurement is, or conversely, how far from the reported value the true (error free) value might be. If the value displayed on your barplot is the result of an aggregation (like the mean value of several data points), you may want to display error bars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have visualized  the data, cleaned them and obtained some insights from them, we would like to compute the summary statistics for each feature. Implement the `sum_stat` function which returnes a dictionary of dictionaries, meaning that a key value of a feature will return a dictionary with keys of min, Q1, median, Q3, max. **Do not use pandas `describe()` function in this implementation**.\n",
    "It should look something like this:\n",
    "\n",
    "d_summary = {<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"MSTV\": {<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"min\": 2.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q1\": 3.0,<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"median\": 4.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q3\": 5.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"max\": 6.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;},<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"LB\": {<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"min\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q1\": ...,<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"median\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q3\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"max\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;},<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;}<br>\n",
    "<br> \n",
    "You can access with : d_summary[\"MSTV\"][\"min\"] = 2.0\n",
    "\n",
    "You can use that output in order to have another cleanup and this time, it will be a cleanup of outliers. We will stick to the definition of an outlier according to the five number summary that are actually represented by boxplots. Just as a reminder and comparison to a normal distribution, have a look at the next figure:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*NRlqiZGQdsIyAu0KzP7LaQ.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import sum_stat as sst\n",
    "d_summary = sst(c_samp)\n",
    "print(d_summary['MSTV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `rm_outlier` that will have the output of `sum_stat` as an input and will return a DataFrame of the that will have outliers removed. In order to avoid issues of different lengths of entries, instead of removing the outliers, simply replace them with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import rm_outlier\n",
    "c_no_outlier = rm_outlier(c_samp, d_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the features `Median`, `Mean` and `Mode` for comparioson previous to outliers removal and after it using boxplots. First we plot the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_samp.boxplot(column=['Median','Mean','Mode'])\n",
    "plt.ylabel('Fetal Heart Rate [bpm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the \"clean data\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_no_outlier.boxplot(column=['Median','Mean','Mode'])\n",
    "plt.ylabel('Fetal Heart Rate [bpm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q3:** Boxplotting the data after the outliers were removed shows us that there are no outliers anymore. Is it necessarily always the case, meaning if you take the \"clean\" data and boxplot it again will it have no outliers for sure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q3:** When reviewing a box plot, an outlier is defined as a data point that is located outside the whiskers of the box plot. For example, outside 1.5 times the interquartile range above the upper quartile and below the lower quartile (Q1 - 1.5 * IQR or Q3 + 1.5 * IQR). Removing the outliers will give us a new set of data, so we will need to calculate Q1, Q3 and IQR again, this  may create new outliers depends on the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is one more thing that should be reminded in the aspect of data exploration and it is the second rule of this field that states the following: **USE COMMON SENSE!**\n",
    "\n",
    "What it really means is that if you have some physiological prior (e.g. you know the range of values of your features), so you should have some sanity checks. For example, the feature `LB` tells us about the FHR baseline. It won't make any sense if we found values that are higher than 500 bpm, not even mentioning non-positive values. Your next mission is to implement the function `phys_prior` where you choose one feature (which is not `LB`), explain what you think it's normal range is and clean it accordingly. The function will have `c_samp`, the feature that you chose and a threshold as inputs. The explanation should be written shortly as a comment next to the input as you can see at the following cell. The only lines you can change here are the `feature` value, the `thresh` value and its comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import phys_prior as phpr\n",
    "feature = 'FM'\n",
    "'''\n",
    "fetal movement (FM) in healthy baby vary from 4–100 per hour, so if we determine our tresh to be 100 , we will be in the normal range.\n",
    "'''\n",
    "thresh = 100\n",
    "filt_feature = phpr(c_samp, feature, thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling: Standardization and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point you have successfully cleaned your dataset, well done! The clean dataset was saved in pickle format called `objs.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('objs.pkl', 'rb') as f:\n",
    "    CTG_features, CTG_morph, fetal_state = pickle.load(f)\n",
    "orig_feat = CTG_features.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will address an important step in data science which is called feature scaling. Here we will discuss about standardization and normalization. As you saw in the lectures, scaling enables us to prepare features that take their value in different ranges and map them to a “normalized” features that take their values in similar ranges.\n",
    "Implement the methods of the class `NSD`. The `transform` method function will return the data normalized/standardized according to the mode. You should also choose two features for comparison (using histograms) between the original data and the different modes. The two histograms should be plotted at the same figure with different colors as you saw in the lecture. You should have one figure per `mode` plus original data so ath the next cell you would end up with 4 figures having two histograms per figure. Use `matplotlib` as you saw in your tutorials. The argument `flag` is used for visibility of histograms.  There are three types of `mode`: `'standard','MinMax' and 'mean'` as shown in the lecture. You may change `selected_feat` in the next cell if youl'd like to see that your implemetations apply well on other features as well.\n",
    "\n",
    "\n",
    "**Notice:**\n",
    "\n",
    "1. `fit` method should calculate all needed values for all possible modes. The mode is chosen only in `transform` (or `fit_transform`).\n",
    "\n",
    "\n",
    "2. Once you apply the transformation, it should return a datframe which all of its samples in all of of the features are scaled correctly according to the mode. The chioce of `selected_feat` is only for those features that you would like to plot their histograms but all of the features have to be scaled regardless the ones you choose to plot.\n",
    "**Hint**: Use of `df.apply(np.mean)` for example would return a series that could be broadcasted easily with df1 for different calculations. Thus, `df - df.apply(np.mean)` would return a Dataframe where the **adequate** mean value was extracted, for instance the `LB` series will now be `LB-mean(LB)`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import NSD\n",
    "selected_feat = ('LB','ASTV')\n",
    "scaler = NSD()\n",
    "plt.figure(1)\n",
    "orig = scaler.fit_transform(CTG_features, selected_feat=selected_feat, flag=True)\n",
    "plt.figure(2)\n",
    "nsd_std = scaler.fit_transform(CTG_features, mode='standard', selected_feat=selected_feat, flag=True)\n",
    "plt.figure(3)\n",
    "nsd_norm = scaler.fit_transform(CTG_features, mode='MinMax', selected_feat=selected_feat, flag=True)\n",
    "plt.figure(4)\n",
    "nsd_norm_mean = scaler.fit_transform(CTG_features, mode='mean', selected_feat=selected_feat, flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q4:** Explain why normalization is not useful when there are outliers with extremely large or small values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q4:** Extremely large or small values affect the min, max and mean of the data and because of that they may affect the normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after all of the hard work we can now harvest the fruits (your functions from Part I) in order to do some proper machine learning!  \n",
    "\n",
    "Note: It is recommended that you attend the second workshop for this part and use the notes in your homework folder.\n",
    "\n",
    "In this assignment we will assume that our data is linearly separable and we will use logistic regression as our classification method. In other words, we choose a linear hypothesis class function . We would try to make the separation in the feature domain (i.e. our graph axes are the features) and we will have a multiclass problem.\n",
    "\n",
    "For every sample (example as called in the lecture) we have two types of labels and we will deal only with one of them. Our goal is to learn the function that gets a sample as an input and returns a predicted value which is supposed to be the calss (label) that it belongs to. Our type of label will be `fetal_state`. Before we continue towards the \"learning\" part we will have another look at our data. Starting with our labels distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# with seaborn:\n",
    "g = sns.countplot(x = 'NSP', data = fetal_state, ax=axes[0])\n",
    "g.set(xticklabels=['Normal','Suspect','Pathology'])\n",
    "# with matplotlib\n",
    "fetal_state.NSP.value_counts().plot(kind=\"pie\", labels=['Normal','Suspect', 'Patholgy'], autopct='%1.1f%%', ax=axes[1])\n",
    "plt.show()\n",
    "idx_1 = (fetal_state == 1).index[(fetal_state == 1)['NSP'] == True].tolist()\n",
    "idx_2 = (fetal_state == 2).index[(fetal_state == 2)['NSP'] == True].tolist()\n",
    "idx_3 = (fetal_state == 3).index[(fetal_state == 3)['NSP'] == True].tolist()\n",
    "print(\"Normal samples account for \" + str(\"{0:.2f}\".format(100*len(idx_1)/len(fetal_state))) + \"% of the data.\")\n",
    "print(\"Suspect samples account for \" + str(\"{0:.2f}\".format(100*len(idx_2)/len(fetal_state))) + \"% of the data.\")\n",
    "print(\"Pathology samples account for \" + str(\"{0:.2f}\".format(100*len(idx_3)/len(fetal_state))) + \"% of the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the CTG's were labeled as normal. Mostly, lables are made by professionals (in our case, doctors) based on the interpretation of the FHR and our job is to make the computer make the same decisions as a doctor would do but automatically. Now let's get the feeling of how well the features correlate with the labels and with one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 100\n",
    "feat = 'Width'\n",
    "plt.hist(CTG_features[feat].loc[idx_1], bins, density=True, alpha=0.5, label='Normal')\n",
    "plt.hist(CTG_features[feat].loc[idx_2], bins, density=True, alpha=0.5, label='Suspect')\n",
    "plt.hist(CTG_features[feat].loc[idx_3], bins, density=True, alpha=0.5, label='Pathology')\n",
    "plt.xlabel('Histogram Width')\n",
    "plt.ylabel('Probabilty')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(CTG_features[['LB','AC','FM','UC']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q5:** What information does feature-feature correlation provide?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q5:** Feature-feature correlation is used to find the correlation between both features in order to get a correlation between every couple of features, this helps us see how the features are related to each other, and we can examine the posibility of linear relation between the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we are pretty much done with data exploration. Now the learning part begins. As you saw in the tutorials, one of the most common and useful packages that Python has to offer in the learning field is `sklearn` package. The first thing you need to do after exploring your data is to divide it into 2 sets: `training set` and `test set`. As a rule of thumb, a typical split of your dataset is 80%, 20%  respectively. Later on we will also use validation set.\n",
    "One of the most common linear classification models is logistic regression – abbreviated ‘LR’. We will use this model through our assignment from now on. \n",
    "Implement the function `pred_log` which is in the module `lin_classifier`. It should return a tuple of two elements. The first one is a vector of predicted classes and the other one is the weighting matrix (w) that should have the shape of (# of classes, # of features).\n",
    "\n",
    "As you noticed, most of our data is labeled as \"Normal\" which means that our data is *imbalanced*. This is the reason why we *stratification* when we split out data to training and test sets. Stratification means that the split preserves the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lin_classifier import *\n",
    "orig_feat = CTG_features.columns.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(CTG_features, np.ravel(fetal_state), test_size=0.2, \n",
    "                                                    random_state=0, stratify=np.ravel(fetal_state))\n",
    "logreg = LogisticRegression(solver='saga', multi_class='multinomial', penalty='none', max_iter=10000)\n",
    "y_pred, w = pred_log(logreg, X_train, y_train, X_test)\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test, y_pred, average='macro'))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you got about 88% accuracy. Not bad! This is a better result comparing the naive classifier (prediciting a \"Normal\" label for every sample) that should have given us around 77% accuracy. Now we'll practice an interpretation of the results. As you saw in lecture 4, odds ratio can be applied on binary LR, and here we'll expand it to the multinomial (ovr) case. We want to explore how addition of +1 to a specific feature affect the 'Normal' labeling incidence. Implement the function `odds_ratio` under lin_classifier and apply it on `X_test` as given in the next cell. The function inputs are weights (`w, x`) and the selected feature. Choose one of the features as you wish and return both the `odds ratio` and the median `odd`.\n",
    "\n",
    "*Hint:*\n",
    "*First try to understand the relation between binary LR and one-vs-rest LR.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feat = 'LB'\n",
    "# Change x to be either training or testing according to your decision\n",
    "odds, ratio = odds_ratio(w, X_test, selected_feat=selected_feat)  # you have to fill the right X first\n",
    "\n",
    "print(f'The odds ratio of {selected_feat} for Normal is {ratio}')\n",
    "print(f\"The odds to be labeled as 'Normal' are {odds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "**Q6:** What is the meaning of your results? Did you expect this? If not, what could have helped? Explain the difference between odds ratio and odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q6:** Odds is equal to the probabilty of occuring divided by the probabilty of not occuring , in our case it is the probability for normal divided by not normal. Odds represents likelihood of some event to happen. In our case the odds for being labeled as normal are 49.37084. Odds ratio denotes the ratio of the odds. The odds ratio for a variable in LR represents how the odds change with one unit\n",
    "increase in that variable holding all other variables constant. The odds ratio that we get is lower than 1. For one unit increase in LB the odds of being labled as normal will decease by 0.82%. That means the probabilty of belonging to normal will be lower than what it would have been if we would not have increased LB by one unit. However from our analysis from pie chart plotted before we see that most of our samples are labled as normal. Normal samples account for 77.85% of the data. So the weight matrix generated by logistic regression is somewhat counterintuitive and since we were expecting higher number for odds to be labeled as Normal. Use of validation set would have helped us to get more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now let's see if normalization and standardization help us. Fill the next cell and print the accuracy and F1 of the model on the testing set after standardizng and training correctly. *Important notes*:\n",
    "\n",
    "* Avoid information leakage!\n",
    "* Do not apply the `NSD` methods on the labels. \n",
    "* Set the `flag` argument to `False` when using `NSD` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Implement your code here:------------------------------\n",
    "selected_feat = CTG_features.columns\n",
    "CTG_features_std = scaler.fit_transform(CTG_features, mode='standard', selected_feat=selected_feat, flag=False)\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(CTG_features_std, np.ravel(fetal_state), test_size=0.2, \n",
    "                                                    random_state=0, stratify=np.ravel(fetal_state))\n",
    "logreg_new = LogisticRegression(solver='saga', multi_class='multinomial', penalty='none', max_iter=10000)\n",
    "y_pred_new, w_new = pred_log(logreg_new, X_train_new, y_train_new, X_test_new)\n",
    "print(\"Standardization :\\n\")\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test_new, y_pred_new))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test_new, y_pred_new, average='macro'))) + \"%\")\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTG_features_norm = scaler.fit_transform(CTG_features, mode='mean', selected_feat=selected_feat, flag=False)\n",
    "\n",
    "X_train_new_2, X_test_new_2, y_train_new_2, y_test_new_2 = train_test_split(CTG_features_norm, np.ravel(fetal_state), test_size=0.2, \n",
    "                                                    random_state=0, stratify=np.ravel(fetal_state))\n",
    "logreg_new_2 = LogisticRegression(solver='saga', multi_class='multinomial', penalty='none', max_iter=10000)\n",
    "y_pred_new_2, w_new_2 = pred_log(logreg_new_2, X_train_new_2, y_train_new_2, X_test_new_2)\n",
    "print(\"Normalization :\\n\")\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test_new_2, y_pred_new_2))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test_new_2, y_pred_new_2, average='macro'))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now choose one of the modes and stick to it. Let's visualize our learned parameters. Use your chosen weight matrix as an input to the function `w_no_p_table` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change input_mat to your weight matrix from the cell above\n",
    "w_no_p_table(w_new,orig_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q7:** Mention one advantage and one limitation of using cross entropy.\n",
    "\n",
    "**Q8:** By selecting one feature at a time and compare their learned weights by looking at plots we had, what can you tell about the weights relations? Why does it happen? **Hint:** notice the sign of the weights and remember that an exponent is a monotonic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q7:** \n",
    "Cross-entropy loss is used when adjusting model weights during training. The aim is to minimize the loss, i.e, the smaller the loss the better the model. A perfect model has a cross-entropy loss of 0.\n",
    "\n",
    "Advantages - \n",
    "1) One advantage is that using it makes it easier to reach a global minima due to its convexity.\n",
    "2) Cross entropy loss is used to simplify the derivative of the softmax function. \n",
    "3) It is very intuitive to work with entropy (information content) when handling probabilities, but more importantly the cross entropy has some nice properties, especially for sigmoid activation functions.\n",
    "\n",
    "Disadvantages - \n",
    "1) Minimizing the cross-entropy loss by using a gradient method could lead to a very poor margin if the features of the dataset lie on a low-dimensional subspace. \n",
    "2) Other limitation is that it's binary so it gives only the values 0 and 1.\n",
    "3) It is also hard to compute and takes computational resources if there are many features in your dataset resulting your dimensional space becoming too big.\n",
    "\n",
    "**Q8:** Regression analysis seeks to define the relationship between a dependent variable (y) and any number of independent variables (X). Logistic regression coefficients(or the weight matrix) represent the log odds that an observation is in the target class (“1”) given the values of its X variables. Thus, these log odd coefficients need to be converted to regular odds in order to make sense of them. Taking the exponent of weight matrix gives us the odds matrix, so if the variable w is positive then the odds for the feature to be labeled as the label suggests is high, and if it is negative the odds will be lower. According to our results, we can see that the sign of the w in the normal case is the opposite than the other cases, thats mean if the odd of a feature to be normal is positive (its odd is big) the w for the same feature to be not normal will have a negative value, the odd for this feature to be labeled as suspedcted or pathology will be small. We can see that sum of weight is zero for every feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's recall that in the lecture you saw that accuracy is not always our best measure. Sensitivity and specificity can be much more informative and important mostly. The choise to train a model to have better results in sensitivity aspect rather than specificty or vice versa depends on our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal','Suspect','Pathology'], yticklabels=['Normal','Suspect','Pathology'])\n",
    "ax.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q9:** What do you think is more important to us with this data? What is the clinical risk/cost for False Positive (FP) and False Negative (FN)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q9:** We think that sensitivity is more important than specificity. Higher sensitivity means less false negative cases, we won't want to miss pathology cases which may be risky, because of that the clinical cost for false positive is leading to more diagnostic processes which take time and waste money. On the other hand, detecting false negative can put the patient in risk because we are ignoring a disease that exists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we will try to handle one of the main issues in learning which is called **overfitting** and one way to deal with it is called **regularization**.\n",
    "\n",
    "There are several types of regularizations and in this assignment we will expirience two of them:\n",
    "\n",
    "1) Loss regularization.\n",
    "\n",
    "2) Validation.\n",
    "\n",
    "The loss function is a function that takes the predicted values and the labels and *measures* how \"close\" they are to each other. Our demand is that this \"distance\" (metric) would be as low as possible. In addition to it, we can add more \"demands\" that we can represent by mathematical terms. For example, we can demand that the number of coefficients won't get to large or we can try to restrict their values. A more physical example is a demand that our signal has to be smooth. When we try to minimize the new loss function we actually try to find the result which is the comprimise of our demands. We can also \"favour\" one demand over another using **regularization parameters**.\n",
    "\n",
    "You saw in the lecture \"demands\" on the learned weights and represented those demands mathematically using $ L_1 $ and $ L_2 $ norms. The regularization parameter was denoted as $\\lambda$ (please notice that sometimes it is common to use the notation of $ c $ where $\\lambda = c^{-1}$). Now it's your turn to become artists! Change and/or add arguments to *LogisticRegression* class in the next cell and perform learning using two regularizations: $ L_1 $ and $ L_2 $. Examine your results using the confusion matrix as we did before. Tune your regularization parameter until you get a result that you think is reasonable and that it brings the sensitivity/specificity (depending on what you chose before) to the maximum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------- Implement your code here:----------------------------------------------------------\n",
    "mode = 'MinMax'\n",
    "X_train_scaled=scaler.transform(X_train).dropna()\n",
    "X_test_scaled=scaler.fit_transform(X_test).dropna()\n",
    "\n",
    "lmda_l1 = 0.2\n",
    "C_l1=1/lmda_l1\n",
    "\n",
    "logreg_l1 = LogisticRegression(penalty='l1', solver='saga', C=C_l1, multi_class='multinomial', max_iter=10000) # L1\n",
    "y_pred_1, w1 = pred_log(logreg_l1, X_train_scaled, y_train, X_test_scaled) \n",
    "\n",
    "matrix1 = metrics.confusion_matrix(y_test, y_pred_1)\n",
    "subplot1 = plt.subplot(211)\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal','Suspect','Pathology'], yticklabels=['Normal','Suspect','Pathology'])\n",
    "subplot1.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "\n",
    "lmda_l2 = 0.2\n",
    "C_l2=1/lmda_l2\n",
    "\n",
    "logreg_l2 = LogisticRegression(penalty='l2', solver='saga', C=C_l2 , multi_class='multinomial', max_iter=10000) # L2\n",
    "y_pred_l2, w2 = pred_log(logreg_l2, X_train_scaled, y_train, X_test_scaled) \n",
    "\n",
    "matrix = metrics.confusion_matrix(y_test, y_pred_l2)\n",
    "subplot2 = plt.subplot(212)\n",
    "sns.heatmap(matrix, annot=True, xticklabels=['Normal','Suspect','Pathology'], yticklabels=['Normal','Suspect','Pathology'])\n",
    "subplot2.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "plt.show()\n",
    "#----------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are happy with your results, let's compare the coefficients of the two norms. Choose two weightning matrices (one calculated using $ L_2 $ and the other calculated using $ L_1 $) and use them as inputs in `w_all_tbl` function. This function sorts the weights according to their $ L_2 $ norm (so the first argument has to be the matrix of $ L_2 $) and compares them to $ L_1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_all_tbl(w2, w1, orig_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the features are ordered differently because they are sorted according to $ L_2 $ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q10:** What is the difference that you can see when plotting $ L_1 $ vs. $ L_2 $? Could you expect it ahead?\n",
    "\n",
    "**Q11:** Why is LASSO coupled with feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q10:** According to the results the w value of the L1 is always higher that the w value of the L2.\n",
    "From the expression in the lecture we can see that for L1 q1=1 and for L2 q2=2 and for the squared value for L2, we could expect to get a higher value for L1 before we got the results.\n",
    "\n",
    "\n",
    "**Q11:** LASSO involves a penalty factor that determines how many features are retained, using cross-validation to choose the penalty factor helps assure that the model will generalize well to future data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a method that help us to choose what we call *hyperparameters* of the model. This is also a method of regularization and it is called **validation**. There are several types of validation and here we will use *stratified K-fold cross validation*. The hyperparmeters that we would like to choose are the norms that we want to train with and the regularization parameter. Again, we use stratification for the folds to prevent biased learning.\n",
    "\n",
    "Implement the function `cv_kfold` in `lin_classifier` module. We will use `X_train` as our training set that will be iteratively divided into $ K-1 $ training sets and one validation set. **Notice:** choose wisely where to apply `NSD` fit and transformation to avoid information leakage in every iteration. In this function you should build a list of dictionaries called `validation_dict` where each element in the list contains a dictionary with 4 keys name: `C, penalty, mu and sigma`. For every pair of parameters (`C and penalty`) you will run $ K $ validations and `mu and sigma` will be calculated as the average loss and standard deviation over $ K $ folds respectively. Use the function `log_loss` from `sklearn.metrics` that was already imported in `lin_classifier`. One more thing, you will have to implement a simple modification to `pred_log` function using the `flag` argument. When this flag is set to `True`, the function should return the probabilities of the classes and not the classes themselves. This is the output that `log_loss` function expects to get.\n",
    "In the next cell, build a list called `C` that has up to 6 regularization parameters (different lambda). Set `K` as number of folds. Choose `mode` for `NSD` and set `penalty = ['l1,'l2]`. Then, excecute the function `cv_kfold` which should return the list of dictionaries named `val_dict` that will be used in the following cell.\n",
    "This function might take a while to perform depending on $ K $ and the number of regularization parameters you will choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------Implement your code here:----------------------------------------\n",
    "C = [0.2,0.6,0.8,1.5,8,40]\n",
    "K =  3\n",
    "mode =  'MinMax'\n",
    "val_dict = cv_kfold(X_train, y_train, C=C, penalty=['l1', 'l2'], K=K,mode=mode)\n",
    "#------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "cm = plt.get_cmap('nipy_spectral')\n",
    "for i, d in enumerate(val_dict):\n",
    "    x = np.linspace(0, d['mu'] + 3 * d['sigma'], 1000)\n",
    "    ax = plt.plot(x,stats.norm.pdf(x, d['mu'], d['sigma']), label=\"p = \" + d['penalty'] + \", C = \" + str(d['C']), color=cm(i*20))\n",
    "    plt.title('Gaussian distribution of the loss')\n",
    "    plt.xlabel('Average loss')\n",
    "    plt.ylabel('Probabilty density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now choose parameters according to the results. Explain why did you choose these hyperparmeters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train you model with the **full training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------Implement your code here:----------------------------------------\n",
    "C =  5\n",
    "penalty =  'l1'\n",
    "logreg = LogisticRegression(solver='saga', multi_class='ovr', penalty=penalty, C=C, max_iter=10000)\n",
    "y_pred, w = pred_log(logreg,scaler.transform(X_train,mode=mode).dropna(), y_train, scaler.transform(X_test,mode=mode).dropna())\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "ax1 = plt.subplot(211)\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal','Suspect','Pathology'], yticklabels=['Normal','Suspect','Pathology'])\n",
    "ax1.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "plt.show()\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test, y_pred, average='macro'))) + \"%\")\n",
    "#------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! So as you can see results did get better but not by that much but you got the feeling how to handle with data, what are the basics of learning and what are the real effects and applications of what you saw in the lectures. Now, one last thing: A possible reason for the poor improvements is that our data is probably not linearly separable and we used a linear classifier. There are two basic approaches for this kind of problem:\n",
    "The first one is to use non-linear classifier and the second one is to perform a transformation on our data so it will become linearly separable in another space. Here is an example of 2D data that can also visualize the problem and the second approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sthalles.github.io/assets/fisher-ld/feature_transformation.png\" width=600 align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the features were non-linearly transformed simply by squaring each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q12:** Look at the data. Why was it reasonable to expect that squaring each feature would make the data linearly sperable?\n",
    "\n",
    "**Q13:** Sugest another non-linear transformation (where **both** new axes are a function of **both** $(x_1,x_2)$ that would make the data linearly seperable so that the line that seperates the two data types will be perpendicular to one of the new axes. Write the new two features (axes) **explicitly** as a funciton of $ (x_1,x_2) $. Use LaTex to write mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q12:** We say that two sets are linearly separable precisely when their respective convex hulls are disjoint (colloquially, do not overlap). Looking at the data points ploted on original space we see that it forms circular shape on (x1,x2). We see that each point in this space corresponds to a possible input vector. Once we’ve chosen the weights w and bias b, we can divide the data space into a region where the points are classified as positive (the positive region), and a region where the points are classified as negative (the negative region). The boundary between these regions, i.e. the set where wT x + b = 0, is called the decision boundary. The set determined by this equation is a hyperplane. However when we plot examples in two dimensions, the hyperplanes are actually\n",
    "lines. But we shouldn’t think of them as lines — we should think of them as hyperplanes. Now squaring each point along y and x axis in 2-D plane is similar to folding the 3d hyperplane in convex paraboloid and then projecting it back to 2d plane. Therefore it was reasonable to expect that squaring each feature would make the data linear because the original data are in shape of circle in 2-D plane. So squaring the features gives us a linear relation between the squared values according to the mathematical equation that describes the circle and parabola. We can also think circle as a cross-section of paraboloid.\n",
    "\n",
    "\n",
    "**Q13:** Another non-linear transformation (where **both** new axes are a function of **both** $(x_1,x_2)$ that would make the data linearly seperable can be given as - $(x_1^2 - x_2^2)$ and $(x_1^2 + x_2^2)$ where either one can be x/y-axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just to get the feeling of better results when we go non-linear, let's try the random forest classifier. All you have to do is just choose one of the modes of the `NSD` tranformation method and see if you got better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "mode = 'MinMax' # choose your method\n",
    "clf = rfc(n_estimators=10)\n",
    "clf.fit(scaler.fit_transform(X_train, mode=mode), y_train)\n",
    "y_pred = clf.predict(scaler.transform(X_test, mode=mode))\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal', 'Suspect', 'Pathology'],\n",
    "            yticklabels=['Normal', 'Suspect', 'Pathology'])\n",
    "ax.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "plt.show()\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test, y_pred, average='macro'))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bm-336546')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab71544c22db40a01d8bd10f5d0a0494a307bb8d8066be7271d75cb167e8b777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
